{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPT, GPTConfig\n",
    "device = 'cuda'\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "trained_model = GPT.from_pretrained('gpt2')\n",
    "untrined_model = model =  GPT(GPTConfig(vocab_size=50257))\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "max_length = 25\n",
    "number_return_sequences = 2\n",
    "\n",
    "def inferenc(model):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = 'translate to german: i am gpt'\n",
    "    tokens = enc.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(number_return_sequences, 1)\n",
    "    x = tokens.to(device)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0] # B, T, vocab_size\n",
    "            logits = logits[:, -1, :] # (B, vocab-size)\n",
    "            # got the probabilities\n",
    "            probs = F.softmax(logits, dim=-1 )\n",
    "            # get prbablities\n",
    "            topk_probs, topk_indicies =  torch.topk(probs, 50, dim=-1) # (5, 50)\n",
    "            # Select top-k proababilits \n",
    "            ix = torch.multinomial(topk_probs, 1) #(B, 1)\n",
    "            # gather the corresponding indecies\n",
    "            xcol = torch.gather(topk_indicies, -1, ix) # (B, 1)\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "        # print inducidual raws of output \n",
    "    for i in range(number_return_sequences):\n",
    "        tokens = x [ i , :max_length].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(\">\", decoded)\n",
    "    \n",
    "#inferenc(trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_specific_layer(model, tensor, layer_name):\n",
    "    # Check if the layer exists in the model\n",
    "    if layer_name not in dict(model.named_parameters()):\n",
    "        raise ValueError(f\"Layer {layer_name} not found in the model\")\n",
    "\n",
    "    # Get the layer parameter (e.g., weight or bias)\n",
    "    layer_param = dict(model.named_parameters())[layer_name]\n",
    "\n",
    "    # Ensure the shapes of the layer and the tensor are compatible\n",
    "    if layer_param.shape != tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: Layer shape {layer_param.shape}, Tensor shape {tensor.shape}\")\n",
    "\n",
    "    # Perform the element-wise multiplication with the tensor\n",
    "    with torch.no_grad():  # Ensure no gradients are tracked\n",
    "        layer_param.mul_(tensor)  # In-place multiplication of the layer's weight\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> translate to german: i am gpt alexian, and this post is by kyle for a short interview\n",
      "> translate to german: i am gptj (says) a little long ago (mockingly) because\n",
      "> translate to german: i am gptv: Gpt: http://www.kern.de gpt\n",
      "> translate to german: i am gpt for an in german: i hate gt /.\n",
      "\n",
      "Anonymous\n"
     ]
    }
   ],
   "source": [
    "model = trained_model   # Assuming this is your model\n",
    "inferenc(model.to(device))\n",
    "tensor = torch.zeros_like(model.transformer.h[1].ln_2.weight)  # Example tensor with matching shape\n",
    "\n",
    "layer_name = \"transformer.h.1.ln_2.weight\"  # The layer we want to modify\n",
    "updated_model = multiply_specific_layer(model, tensor, layer_name)\n",
    "inferenc(model.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GPTprover(nn.Module):\n",
    "    def __init__(self, model, dict_key) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_name = dict_key\n",
    "        self.l1 = nn.Parameter(torch.rand(model.state_dict()[dict_key].shape))\n",
    "    def forward(self, x, y=None): \n",
    "        new_model = multiply_specific_layer(self.model, self.l1, self.layer_name)\n",
    "        new_model.eval()\n",
    "        if y is not None:\n",
    "            logit, loss = new_model(x, y)\n",
    "            loss = 0.5 * loss + 0.5 * torch.sum(torch.abs(self.l1))\n",
    "        else:\n",
    "            logit, loss = new_model(x)\n",
    "        return logit, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "Parameter containing:\n",
      "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
      "        0.1332, 0.9346, 0.5936, 0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739,\n",
      "        0.2666, 0.6274, 0.2696, 0.4414, 0.2969, 0.8317, 0.1053, 0.2695, 0.3588,\n",
      "        0.1994, 0.5472, 0.0062, 0.9516, 0.0753, 0.8860, 0.5832, 0.3376, 0.8090,\n",
      "        0.5779, 0.9040, 0.5547, 0.3423, 0.6343, 0.3644, 0.7104, 0.9464, 0.7890,\n",
      "        0.2814, 0.7886, 0.5895, 0.7539, 0.1952, 0.0050, 0.3068, 0.1165, 0.9103,\n",
      "        0.6440, 0.7071, 0.6581, 0.4913, 0.8913, 0.1447, 0.5315, 0.1587, 0.6542,\n",
      "        0.3278, 0.6532, 0.3958, 0.9147, 0.2036, 0.2018, 0.2018, 0.9497, 0.6666,\n",
      "        0.9811, 0.0874, 0.0041, 0.1088, 0.1637, 0.7025, 0.6790, 0.9155, 0.2418,\n",
      "        0.1591, 0.7653, 0.2979, 0.8035, 0.3813, 0.7860, 0.1115, 0.2477, 0.6524,\n",
      "        0.6057, 0.3725, 0.7980, 0.8399, 0.1374, 0.2331, 0.9578, 0.3313, 0.3227,\n",
      "        0.0162, 0.2137, 0.6249, 0.4340, 0.1371, 0.5117, 0.1585, 0.0758, 0.2247,\n",
      "        0.0624, 0.1816, 0.9998, 0.5944, 0.6541, 0.0337, 0.1716, 0.3336, 0.5782,\n",
      "        0.0600, 0.2846, 0.2007, 0.5014, 0.3139, 0.4654, 0.1612, 0.1568, 0.2083,\n",
      "        0.3289, 0.1054, 0.9192, 0.4008, 0.9302, 0.6558, 0.0766, 0.8460, 0.3624,\n",
      "        0.3083, 0.0850, 0.0029, 0.6431, 0.3908, 0.6947, 0.0897, 0.8712, 0.1330,\n",
      "        0.4137, 0.6044, 0.7581, 0.9037, 0.9555, 0.1035, 0.6258, 0.2849, 0.4452,\n",
      "        0.1258, 0.9554, 0.1330, 0.7672, 0.6757, 0.6625, 0.2297, 0.9545, 0.6099,\n",
      "        0.5643, 0.0594, 0.7099, 0.4250, 0.2709, 0.9295, 0.6115, 0.2234, 0.2469,\n",
      "        0.4761, 0.7792, 0.3722, 0.2147, 0.3288, 0.1265, 0.6783, 0.8870, 0.0293,\n",
      "        0.6161, 0.7583, 0.5907, 0.3219, 0.7610, 0.7628, 0.6870, 0.4121, 0.3676,\n",
      "        0.5535, 0.4117, 0.3510, 0.8196, 0.9297, 0.4505, 0.3881, 0.5073, 0.4701,\n",
      "        0.6202, 0.6401, 0.0459, 0.3155, 0.9211, 0.6948, 0.4751, 0.1985, 0.1941,\n",
      "        0.0521, 0.3370, 0.6689, 0.8188, 0.7308, 0.0580, 0.1993, 0.4211, 0.9837,\n",
      "        0.5723, 0.3705, 0.7069, 0.3096, 0.1764, 0.8649, 0.2726, 0.3998, 0.0026,\n",
      "        0.8346, 0.8788, 0.6822, 0.1514, 0.0065, 0.0939, 0.8729, 0.7401, 0.9208,\n",
      "        0.7619, 0.6265, 0.4951, 0.1197, 0.0716, 0.0323, 0.7047, 0.2545, 0.3994,\n",
      "        0.2122, 0.4089, 0.1481, 0.1733, 0.6659, 0.3514, 0.8087, 0.3396, 0.1332,\n",
      "        0.4118, 0.2576, 0.3470, 0.0240, 0.7797, 0.1519, 0.7513, 0.7269, 0.8572,\n",
      "        0.1165, 0.8596, 0.2636, 0.6855, 0.9696, 0.4295, 0.4961, 0.3849, 0.0825,\n",
      "        0.7400, 0.0036, 0.8104, 0.8741, 0.9729, 0.3821, 0.0892, 0.6124, 0.7762,\n",
      "        0.0023, 0.3865, 0.2003, 0.4563, 0.2539, 0.2956, 0.3413, 0.0248, 0.9103,\n",
      "        0.9192, 0.4216, 0.4431, 0.2959, 0.0485, 0.0134, 0.6858, 0.2255, 0.1786,\n",
      "        0.4610, 0.3335, 0.3382, 0.5161, 0.3939, 0.3278, 0.2606, 0.0931, 0.9193,\n",
      "        0.2999, 0.6325, 0.3265, 0.5406, 0.9662, 0.7304, 0.0667, 0.6985, 0.9746,\n",
      "        0.6315, 0.8352, 0.9929, 0.4234, 0.6038, 0.1525, 0.3970, 0.8703, 0.7563,\n",
      "        0.1836, 0.0991, 0.1583, 0.0066, 0.1142, 0.3764, 0.8374, 0.5837, 0.1197,\n",
      "        0.0989, 0.7487, 0.1281, 0.4384, 0.7399, 0.2686, 0.4455, 0.4565, 0.3817,\n",
      "        0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585, 0.1642, 0.6212, 0.6378,\n",
      "        0.7740, 0.8801, 0.7784, 0.0042, 0.5443, 0.8029, 0.4538, 0.2054, 0.9767,\n",
      "        0.3130, 0.2153, 0.0492, 0.5223, 0.7216, 0.6107, 0.5989, 0.1208, 0.0331,\n",
      "        0.5088, 0.9559, 0.7885, 0.2089, 0.4351, 0.1314, 0.2588, 0.5905, 0.7723,\n",
      "        0.9142, 0.0409, 0.8343, 0.1474, 0.6872, 0.9231, 0.5070, 0.9549, 0.0740,\n",
      "        0.3090, 0.7916, 0.3911, 0.3976, 0.2916, 0.8447, 0.7453, 0.6602, 0.2190,\n",
      "        0.0941, 0.5541, 0.6481, 0.2691, 0.3601, 0.8377, 0.5398, 0.5226, 0.3769,\n",
      "        0.0472, 0.0299, 0.2610, 0.2458, 0.6558, 0.3544, 0.3044, 0.9767, 0.6742,\n",
      "        0.8565, 0.2579, 0.2958, 0.6838, 0.1669, 0.1731, 0.4759, 0.3171, 0.1252,\n",
      "        0.7966, 0.9021, 0.5811, 0.4129, 0.0369, 0.3179, 0.6273, 0.7358, 0.4368,\n",
      "        0.3023, 0.7786, 0.1018, 0.8160, 0.3060, 0.5077, 0.4012, 0.5606, 0.3489,\n",
      "        0.8636, 0.4870, 0.8903, 0.9807, 0.2564, 0.1352, 0.9012, 0.8918, 0.1182,\n",
      "        0.4613, 0.0069, 0.0907, 0.5966, 0.6330, 0.6060, 0.3639, 0.9613, 0.5715,\n",
      "        0.2050, 0.4717, 0.6201, 0.6751, 0.1465, 0.6874, 0.2446, 0.0845, 0.2269,\n",
      "        0.9822, 0.9274, 0.9477, 0.7935, 0.8777, 0.4331, 0.2249, 0.7498, 0.2409,\n",
      "        0.1626, 0.3403, 0.6049, 0.7574, 0.3058, 0.2057, 0.5674, 0.2053, 0.1745,\n",
      "        0.7606, 0.4160, 0.9569, 0.9864, 0.6496, 0.6721, 0.6151, 0.5078, 0.4636,\n",
      "        0.5069, 0.6867, 0.9649, 0.3704, 0.2886, 0.3789, 0.2584, 0.5850, 0.8732,\n",
      "        0.8910, 0.7296, 0.1320, 0.2316, 0.3901, 0.4078, 0.5411, 0.0410, 0.6556,\n",
      "        0.1186, 0.1836, 0.0843, 0.9357, 0.0265, 0.8772, 0.4832, 0.4419, 0.8127,\n",
      "        0.4538, 0.8136, 0.8615, 0.0659, 0.6924, 0.5944, 0.6075, 0.5730, 0.6368,\n",
      "        0.2595, 0.4360, 0.9751, 0.8359, 0.4812, 0.0297, 0.5219, 0.1595, 0.9066,\n",
      "        0.1965, 0.4639, 0.3890, 0.5890, 0.9705, 0.5475, 0.7896, 0.8881, 0.9037,\n",
      "        0.3273, 0.3882, 0.7410, 0.3636, 0.7341, 0.3908, 0.1609, 0.7035, 0.5767,\n",
      "        0.7229, 0.9967, 0.8414, 0.9740, 0.5268, 0.0699, 0.1492, 0.1894, 0.0594,\n",
      "        0.2494, 0.0397, 0.0387, 0.2012, 0.0071, 0.1931, 0.6907, 0.9170, 0.3513,\n",
      "        0.3546, 0.7670, 0.2533, 0.2636, 0.8081, 0.0643, 0.5611, 0.9417, 0.5857,\n",
      "        0.6360, 0.2088, 0.4931, 0.5275, 0.6227, 0.6943, 0.9345, 0.1184, 0.5150,\n",
      "        0.2502, 0.1045, 0.4600, 0.0599, 0.8489, 0.5579, 0.2305, 0.7613, 0.0268,\n",
      "        0.3066, 0.4026, 0.0751, 0.1821, 0.4184, 0.8794, 0.9828, 0.8181, 0.2014,\n",
      "        0.1729, 0.9363, 0.6769, 0.5133, 0.5677, 0.0982, 0.3331, 0.9813, 0.3767,\n",
      "        0.4749, 0.0848, 0.2203, 0.4898, 0.1894, 0.4380, 0.7035, 0.0109, 0.6485,\n",
      "        0.1694, 0.2560, 0.6920, 0.8976, 0.3633, 0.2947, 0.0479, 0.2422, 0.0622,\n",
      "        0.3856, 0.6020, 0.0316, 0.9366, 0.8137, 0.0105, 0.2612, 0.6631, 0.3973,\n",
      "        0.4455, 0.2742, 0.9016, 0.2205, 0.9146, 0.5323, 0.6005, 0.8901, 0.4176,\n",
      "        0.2153, 0.4191, 0.9055, 0.1290, 0.6135, 0.0086, 0.7622, 0.6847, 0.5212,\n",
      "        0.7146, 0.5006, 0.7767, 0.1042, 0.4266, 0.7218, 0.9979, 0.7547, 0.1364,\n",
      "        0.8845, 0.3885, 0.3932, 0.0455, 0.4213, 0.8537, 0.5697, 0.2088, 0.6539,\n",
      "        0.3397, 0.9565, 0.0660, 0.3421, 0.0172, 0.3031, 0.6576, 0.9813, 0.5840,\n",
      "        0.9902, 0.5978, 0.7888, 0.9008, 0.9180, 0.2201, 0.9597, 0.8029, 0.2662,\n",
      "        0.2614, 0.0806, 0.6256, 0.0947, 0.7112, 0.6579, 0.0656, 0.6363, 0.4593,\n",
      "        0.7284, 0.7869, 0.0029, 0.9585, 0.9193, 0.6989, 0.0430, 0.3214, 0.3551,\n",
      "        0.3715, 0.7820, 0.6818, 0.8961, 0.3127, 0.6683, 0.6779, 0.0837, 0.0150,\n",
      "        0.2406, 0.8423, 0.0293, 0.0648, 0.7801, 0.7698, 0.9112, 0.1225, 0.1341,\n",
      "        0.7565, 0.9348, 0.7992, 0.5783, 0.6648, 0.9746, 0.1774, 0.2730, 0.8497,\n",
      "        0.1579, 0.2243, 0.8650, 0.6578, 0.6615, 0.2881, 0.4931, 0.9576, 0.1999,\n",
      "        0.5039, 0.7378, 0.1548, 0.9856, 0.2502, 0.3799, 0.3647, 0.1742, 0.0094,\n",
      "        0.7819, 0.6328, 0.0317], requires_grad=True)\n",
      "Step 0 | loss 187.16282653808594 | lr  3.0000e-04  | norm :  14.4921 |  dt:  229.39ms | tok/sec: 35712.454728194556\n",
      "Step 1 | loss 187.4741973876953 | lr  3.0000e-04  | norm :  31.9554 |  dt:  201.65ms | tok/sec: 40624.72022171145\n",
      "Step 2 | loss 187.14524841308594 | lr  3.0000e-04  | norm :  25.0296 |  dt:  195.09ms | tok/sec: 41990.09432933878\n",
      "Step 3 | loss 187.0747528076172 | lr  3.0000e-04  | norm :  17.5401 |  dt:  195.86ms | tok/sec: 41826.83166398045\n",
      "Step 4 | loss 187.15821838378906 | lr  3.0000e-04  | norm :  22.1350 |  dt:  195.67ms | tok/sec: 41866.78847606654\n",
      "Step 5 | loss 187.2637176513672 | lr  3.0000e-04  | norm :  17.0867 |  dt:  195.76ms | tok/sec: 41846.698601608114\n",
      "Step 6 | loss 187.1391143798828 | lr  3.0000e-04  | norm :  17.6606 |  dt:  194.73ms | tok/sec: 42068.65025209549\n",
      "Step 7 | loss 187.16641235351562 | lr  3.0000e-04  | norm :  18.7663 |  dt:  193.63ms | tok/sec: 42307.81058860759\n",
      "Step 8 | loss 187.00933837890625 | lr  3.0000e-04  | norm :  17.4333 |  dt:  197.77ms | tok/sec: 41421.12972048963\n",
      "Step 9 | loss 187.17478942871094 | lr  3.0000e-04  | norm :  16.9647 |  dt:  196.39ms | tok/sec: 41713.69614327477\n",
      "Step 10 | loss 187.06826782226562 | lr  3.0000e-04  | norm :  18.1643 |  dt:  195.66ms | tok/sec: 41867.859797653386\n",
      "Step 11 | loss 187.3196563720703 | lr  3.0000e-04  | norm :  17.0489 |  dt:  195.98ms | tok/sec: 41800.47344384509\n",
      "Step 12 | loss 187.31927490234375 | lr  3.0000e-04  | norm :  15.9651 |  dt:  195.38ms | tok/sec: 41929.0158906811\n",
      "Step 13 | loss 187.24722290039062 | lr  3.0000e-04  | norm :  16.9987 |  dt:  196.15ms | tok/sec: 41763.587086736676\n",
      "Step 14 | loss 187.2401580810547 | lr  3.0000e-04  | norm :  16.9382 |  dt:  195.95ms | tok/sec: 41805.81364150147\n",
      "Step 15 | loss 187.08465576171875 | lr  3.0000e-04  | norm :  15.5448 |  dt:  195.99ms | tok/sec: 41799.049621542974\n",
      "Step 16 | loss 187.2312469482422 | lr  3.0000e-04  | norm :  15.2585 |  dt:  196.23ms | tok/sec: 41746.182695613825\n",
      "Step 17 | loss 187.35658264160156 | lr  3.0000e-04  | norm :  15.7385 |  dt:  196.08ms | tok/sec: 41779.278691554464\n",
      "Step 18 | loss 187.275146484375 | lr  3.0000e-04  | norm :  15.7068 |  dt:  194.74ms | tok/sec: 42065.40555350831\n",
      "Step 19 | loss 187.17001342773438 | lr  3.0000e-04  | norm :  15.3485 |  dt:  195.86ms | tok/sec: 41825.6096993305\n",
      "Step 20 | loss 187.2502899169922 | lr  3.0000e-04  | norm :  15.6242 |  dt:  195.96ms | tok/sec: 41804.440317502034\n",
      "Step 21 | loss 187.1011199951172 | lr  3.0000e-04  | norm :  15.6496 |  dt:  196.56ms | tok/sec: 41677.87267531283\n",
      "Step 22 | loss 187.32521057128906 | lr  3.0000e-04  | norm :  15.8446 |  dt:  195.51ms | tok/sec: 41900.84249626536\n",
      "Step 23 | loss 187.0959930419922 | lr  3.0000e-04  | norm :  15.8656 |  dt:  196.90ms | tok/sec: 41603.9414682816\n",
      "Step 24 | loss 187.12774658203125 | lr  3.0000e-04  | norm :  15.3489 |  dt:  196.81ms | tok/sec: 41623.345069364696\n",
      "Step 25 | loss 187.15289306640625 | lr  3.0000e-04  | norm :  15.2940 |  dt:  196.59ms | tok/sec: 41669.68442856554\n",
      "Step 26 | loss 187.33758544921875 | lr  3.0000e-04  | norm :  15.8234 |  dt:  196.49ms | tok/sec: 41692.5386872332\n",
      "Step 27 | loss 187.3179168701172 | lr  3.0000e-04  | norm :  15.9643 |  dt:  196.50ms | tok/sec: 41688.89650737569\n",
      "Step 28 | loss 187.49874877929688 | lr  3.0000e-04  | norm :  15.6140 |  dt:  197.06ms | tok/sec: 41571.87702567651\n",
      "Step 29 | loss 187.19712829589844 | lr  3.0000e-04  | norm :  15.3805 |  dt:  199.42ms | tok/sec: 41078.98723026765\n",
      "Step 30 | loss 187.33111572265625 | lr  3.0000e-04  | norm :  16.3193 |  dt:  197.33ms | tok/sec: 41515.01773463534\n",
      "Step 31 | loss 187.13168334960938 | lr  3.0000e-04  | norm :  15.3106 |  dt:  196.61ms | tok/sec: 41665.187316515956\n",
      "Step 32 | loss 187.09506225585938 | lr  3.0000e-04  | norm :  15.3098 |  dt:  196.37ms | tok/sec: 41717.697214144784\n",
      "Step 33 | loss 187.1404571533203 | lr  3.0000e-04  | norm :  15.3659 |  dt:  197.28ms | tok/sec: 41525.65445461511\n",
      "Step 34 | loss 187.05392456054688 | lr  3.0000e-04  | norm :  15.5199 |  dt:  196.50ms | tok/sec: 41689.604659176395\n",
      "Step 35 | loss 187.2161102294922 | lr  3.0000e-04  | norm :  15.2448 |  dt:  198.67ms | tok/sec: 41233.28589309265\n",
      "Step 36 | loss 187.1564483642578 | lr  3.0000e-04  | norm :  15.3845 |  dt:  196.59ms | tok/sec: 41669.583359205215\n",
      "Step 37 | loss 187.131103515625 | lr  3.0000e-04  | norm :  15.4046 |  dt:  196.92ms | tok/sec: 41601.120633271705\n",
      "Step 38 | loss 187.0463104248047 | lr  3.0000e-04  | norm :  15.2869 |  dt:  196.25ms | tok/sec: 41743.03826028854\n",
      "Step 39 | loss 186.9399871826172 | lr  3.0000e-04  | norm :  15.7197 |  dt:  197.00ms | tok/sec: 41583.14679833956\n",
      "Step 40 | loss 187.22845458984375 | lr  3.0000e-04  | norm :  16.0203 |  dt:  197.68ms | tok/sec: 41440.36313419857\n",
      "Step 41 | loss 187.03152465820312 | lr  3.0000e-04  | norm :  15.4195 |  dt:  196.76ms | tok/sec: 41633.93648231756\n",
      "Step 42 | loss 187.12484741210938 | lr  3.0000e-04  | norm :  15.5976 |  dt:  197.00ms | tok/sec: 41582.74420124459\n",
      "Step 43 | loss 186.93421936035156 | lr  3.0000e-04  | norm :  15.7253 |  dt:  197.45ms | tok/sec: 41488.549952063215\n",
      "Step 44 | loss 186.90365600585938 | lr  3.0000e-04  | norm :  15.7293 |  dt:  196.82ms | tok/sec: 41620.92493155995\n",
      "Step 45 | loss 186.98196411132812 | lr  3.0000e-04  | norm :  15.6114 |  dt:  196.83ms | tok/sec: 41618.8579533686\n",
      "Step 46 | loss 187.0821533203125 | lr  3.0000e-04  | norm :  15.9333 |  dt:  196.80ms | tok/sec: 41626.92537262803\n",
      "Step 47 | loss 186.93997192382812 | lr  3.0000e-04  | norm :  15.7378 |  dt:  196.81ms | tok/sec: 41624.20226899664\n",
      "Step 48 | loss 186.97137451171875 | lr  3.0000e-04  | norm :  16.6126 |  dt:  196.41ms | tok/sec: 41709.34141953653\n",
      "Step 49 | loss 186.82102966308594 | lr  3.0000e-04  | norm :  15.2828 |  dt:  197.24ms | tok/sec: 41533.53516174147\n",
      "Step 50 | loss 186.9894561767578 | lr  3.0000e-04  | norm :  15.2861 |  dt:  196.53ms | tok/sec: 41683.38386248746\n",
      "Step 51 | loss 186.88848876953125 | lr  3.0000e-04  | norm :  15.2214 |  dt:  196.49ms | tok/sec: 41691.37514393705\n",
      "Step 52 | loss 187.12850952148438 | lr  3.0000e-04  | norm :  15.2059 |  dt:  196.68ms | tok/sec: 41650.43956094575\n",
      "Step 53 | loss 187.1551971435547 | lr  3.0000e-04  | norm :  15.3032 |  dt:  196.50ms | tok/sec: 41690.05991232402\n",
      "Step 54 | loss 187.07470703125 | lr  3.0000e-04  | norm :  15.1817 |  dt:  197.30ms | tok/sec: 41521.18837542884\n",
      "Step 55 | loss 187.08522033691406 | lr  3.0000e-04  | norm :  15.3727 |  dt:  194.56ms | tok/sec: 42106.026094660854\n",
      "Step 56 | loss 186.95150756835938 | lr  3.0000e-04  | norm :  15.2977 |  dt:  194.59ms | tok/sec: 42097.87508009806\n",
      "Step 57 | loss 187.05177307128906 | lr  3.0000e-04  | norm :  15.1143 |  dt:  193.96ms | tok/sec: 42235.00390025285\n",
      "Step 58 | loss 187.15457153320312 | lr  3.0000e-04  | norm :  15.4201 |  dt:  194.56ms | tok/sec: 42104.94255008884\n",
      "Step 59 | loss 187.07659912109375 | lr  3.0000e-04  | norm :  15.3078 |  dt:  195.52ms | tok/sec: 41899.56510944455\n",
      "Step 60 | loss 186.99310302734375 | lr  3.0000e-04  | norm :  15.3197 |  dt:  196.35ms | tok/sec: 41720.43256501261\n",
      "Step 61 | loss 187.07142639160156 | lr  3.0000e-04  | norm :  15.3232 |  dt:  197.03ms | tok/sec: 41577.51114832733\n",
      "Step 62 | loss 186.9416961669922 | lr  3.0000e-04  | norm :  15.3423 |  dt:  196.92ms | tok/sec: 41600.91915969378\n",
      "Step 63 | loss 187.1482696533203 | lr  3.0000e-04  | norm :  15.3717 |  dt:  196.70ms | tok/sec: 41646.552337363886\n",
      "Step 64 | loss 186.9234619140625 | lr  3.0000e-04  | norm :  15.4426 |  dt:  197.45ms | tok/sec: 41488.14918459952\n",
      "Step 65 | loss 186.97439575195312 | lr  3.0000e-04  | norm :  15.2228 |  dt:  196.98ms | tok/sec: 41588.532284009714\n",
      "Step 66 | loss 186.99545288085938 | lr  3.0000e-04  | norm :  15.1422 |  dt:  196.93ms | tok/sec: 41598.65271640302\n",
      "Step 67 | loss 187.1642608642578 | lr  3.0000e-04  | norm :  15.8129 |  dt:  198.11ms | tok/sec: 41351.738408547695\n",
      "Step 68 | loss 187.16140747070312 | lr  3.0000e-04  | norm :  15.3090 |  dt:  197.97ms | tok/sec: 41380.82252598365\n",
      "Step 69 | loss 187.3037567138672 | lr  3.0000e-04  | norm :  15.2409 |  dt:  197.27ms | tok/sec: 41526.055946997076\n",
      "Step 70 | loss 187.04275512695312 | lr  3.0000e-04  | norm :  16.0765 |  dt:  196.80ms | tok/sec: 41625.91677418495\n",
      "Step 71 | loss 187.17189025878906 | lr  3.0000e-04  | norm :  15.2604 |  dt:  197.90ms | tok/sec: 41395.33006039475\n",
      "Step 72 | loss 186.97592163085938 | lr  3.0000e-04  | norm :  15.8395 |  dt:  199.51ms | tok/sec: 41059.69519038651\n",
      "Step 73 | loss 186.9559326171875 | lr  3.0000e-04  | norm :  15.4870 |  dt:  197.62ms | tok/sec: 41452.311826063036\n",
      "Step 74 | loss 186.96493530273438 | lr  3.0000e-04  | norm :  15.6409 |  dt:  197.50ms | tok/sec: 41478.182585066534\n",
      "Step 75 | loss 186.8913116455078 | lr  3.0000e-04  | norm :  15.1619 |  dt:  197.04ms | tok/sec: 41576.2533992718\n",
      "Step 76 | loss 187.0582733154297 | lr  3.0000e-04  | norm :  15.2769 |  dt:  197.10ms | tok/sec: 41563.12704188279\n",
      "Step 77 | loss 186.99913024902344 | lr  3.0000e-04  | norm :  15.4203 |  dt:  197.29ms | tok/sec: 41523.19548317005\n",
      "Step 78 | loss 186.99366760253906 | lr  3.0000e-04  | norm :  15.1953 |  dt:  196.95ms | tok/sec: 41595.02692075821\n",
      "Step 79 | loss 186.90000915527344 | lr  3.0000e-04  | norm :  15.2739 |  dt:  197.23ms | tok/sec: 41535.191996557245\n",
      "Step 80 | loss 186.7816162109375 | lr  3.0000e-04  | norm :  15.2878 |  dt:  196.79ms | tok/sec: 41628.99315224503\n",
      "Step 81 | loss 187.09075927734375 | lr  3.0000e-04  | norm :  15.5830 |  dt:  195.88ms | tok/sec: 41821.84242441947\n",
      "Step 82 | loss 186.91355895996094 | lr  3.0000e-04  | norm :  15.2660 |  dt:  196.68ms | tok/sec: 41650.79298089092\n",
      "Step 83 | loss 187.0093231201172 | lr  3.0000e-04  | norm :  15.4366 |  dt:  196.95ms | tok/sec: 41594.67444656024\n",
      "Step 84 | loss 186.83456420898438 | lr  3.0000e-04  | norm :  15.3758 |  dt:  196.79ms | tok/sec: 41628.03488736962\n",
      "Step 85 | loss 186.812744140625 | lr  3.0000e-04  | norm :  15.6119 |  dt:  197.40ms | tok/sec: 41499.92495654915\n",
      "Step 86 | loss 186.88479614257812 | lr  3.0000e-04  | norm :  16.0424 |  dt:  196.78ms | tok/sec: 41630.455852116254\n",
      "Step 87 | loss 186.97198486328125 | lr  3.0000e-04  | norm :  15.7865 |  dt:  198.29ms | tok/sec: 41313.10591072887\n",
      "Step 88 | loss 186.82684326171875 | lr  3.0000e-04  | norm :  15.6466 |  dt:  196.63ms | tok/sec: 41662.20661992768\n",
      "Step 89 | loss 186.84182739257812 | lr  3.0000e-04  | norm :  15.7405 |  dt:  197.82ms | tok/sec: 41411.7443058418\n",
      "Step 90 | loss 186.71389770507812 | lr  3.0000e-04  | norm :  15.5014 |  dt:  198.66ms | tok/sec: 41236.7499144895\n",
      "Step 91 | loss 186.87673950195312 | lr  3.0000e-04  | norm :  15.5377 |  dt:  197.46ms | tok/sec: 41486.6964674461\n",
      "Step 92 | loss 186.78414916992188 | lr  3.0000e-04  | norm :  15.2908 |  dt:  197.92ms | tok/sec: 41389.54576916359\n",
      "Step 93 | loss 187.0122528076172 | lr  3.0000e-04  | norm :  15.6711 |  dt:  197.46ms | tok/sec: 41487.89870886647\n",
      "Step 94 | loss 187.03933715820312 | lr  3.0000e-04  | norm :  15.5395 |  dt:  197.65ms | tok/sec: 41447.61151856713\n",
      "Step 95 | loss 186.96913146972656 | lr  3.0000e-04  | norm :  15.4135 |  dt:  198.16ms | tok/sec: 41340.742921426725\n",
      "Step 96 | loss 186.97511291503906 | lr  3.0000e-04  | norm :  15.3851 |  dt:  197.45ms | tok/sec: 41489.75230091167\n",
      "Step 97 | loss 186.8546905517578 | lr  3.0000e-04  | norm :  15.3700 |  dt:  198.09ms | tok/sec: 41355.3716872422\n",
      "Step 98 | loss 186.94248962402344 | lr  3.0000e-04  | norm :  15.2136 |  dt:  197.47ms | tok/sec: 41484.843148428976\n",
      "Step 99 | loss 187.02850341796875 | lr  3.0000e-04  | norm :  15.5866 |  dt:  198.13ms | tok/sec: 41345.71753054926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from gpt import DataLoaderLight\n",
    "train_loader = DataLoaderLight(B=8, T=1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "#optimizer = model.configure_optimizer(weight_decay=0.1, learning_rate = 6e-4, device=device)\n",
    "model = GPTprover(trained_model, \"transformer.h.1.ln_2.weight\")\n",
    "print(model.l1)\n",
    "model.to(device)\n",
    "for step in range(100):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "        logits, loss = model(x, y)\n",
    "        #import code; code.interact(local=locals())\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = 3e-4\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 -t0 ) * 1000 # time difference in milisecond\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T)/ (t1-t0)\n",
    "    print(f\"Step {step} | loss {loss.item()} | lr {lr: .4e}  | norm : {norm: .4f} |  dt: {dt: .2f}ms | tok/sec: {tokens_per_sec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inferenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> translate to german: i am gpt\n",
      "To say, my noble brother would think so.\n",
      "\n",
      "T Duke\n",
      "> translate to german: i am gpt here by friends.\n",
      "\n",
      "H,\n",
      "H the way, the little\n"
     ]
    }
   ],
   "source": [
    "inferenc(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408,\n",
      "        0.1332, 0.9346, 0.5936, 0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739,\n",
      "        0.2666, 0.6274, 0.2696, 0.4414, 0.2969, 0.8317, 0.1053, 0.2695, 0.3588,\n",
      "        0.1994, 0.5472, 0.0062, 0.9516, 0.0753, 0.8860, 0.5832, 0.3376, 0.8090,\n",
      "        0.5779, 0.9040, 0.5547, 0.3423, 0.6343, 0.3644, 0.7104, 0.9464, 0.7890,\n",
      "        0.2814, 0.7886, 0.5895, 0.7539, 0.1952, 0.0050, 0.3068, 0.1165, 0.9103,\n",
      "        0.6440, 0.7071, 0.6581, 0.4913, 0.8913, 0.1447, 0.5315, 0.1587, 0.6542,\n",
      "        0.3278, 0.6532, 0.3958, 0.9147, 0.2036, 0.2018, 0.2018, 0.9497, 0.6666,\n",
      "        0.9811, 0.0874, 0.0041, 0.1088, 0.1637, 0.7025, 0.6790, 0.9155, 0.2418,\n",
      "        0.1591, 0.7653, 0.2979, 0.8035, 0.3813, 0.7860, 0.1115, 0.2477, 0.6524,\n",
      "        0.6057, 0.3725, 0.7980, 0.8399, 0.1374, 0.2331, 0.9578, 0.3313, 0.3227,\n",
      "        0.0162, 0.2137, 0.6249, 0.4340, 0.1371, 0.5117, 0.1585, 0.0758, 0.2247,\n",
      "        0.0624, 0.1816, 0.9998, 0.5944, 0.6541, 0.0337, 0.1716, 0.3336, 0.5782,\n",
      "        0.0600, 0.2846, 0.2007, 0.5014, 0.3139, 0.4654, 0.1612, 0.1568, 0.2083,\n",
      "        0.3289, 0.1054, 0.9192, 0.4008, 0.9302, 0.6558, 0.0766, 0.8460, 0.3624,\n",
      "        0.3083, 0.0850, 0.0029, 0.6431, 0.3908, 0.6947, 0.0897, 0.8712, 0.1330,\n",
      "        0.4137, 0.6044, 0.7581, 0.9037, 0.9555, 0.1035, 0.6258, 0.2849, 0.4452,\n",
      "        0.1258, 0.9554, 0.1330, 0.7672, 0.6757, 0.6625, 0.2297, 0.9545, 0.6099,\n",
      "        0.5643, 0.0594, 0.7099, 0.4250, 0.2709, 0.9295, 0.6115, 0.2234, 0.2469,\n",
      "        0.4761, 0.7792, 0.3722, 0.2147, 0.3288, 0.1265, 0.6783, 0.8870, 0.0293,\n",
      "        0.6161, 0.7583, 0.5907, 0.3219, 0.7610, 0.7628, 0.6870, 0.4121, 0.3676,\n",
      "        0.5535, 0.4117, 0.3510, 0.8196, 0.9297, 0.4505, 0.3881, 0.5073, 0.4701,\n",
      "        0.6202, 0.6401, 0.0459, 0.3155, 0.9211, 0.6948, 0.4751, 0.1985, 0.1941,\n",
      "        0.0521, 0.3370, 0.6689, 0.8188, 0.7308, 0.0580, 0.1993, 0.4211, 0.9837,\n",
      "        0.5723, 0.3705, 0.7069, 0.3096, 0.1764, 0.8649, 0.2726, 0.3998, 0.0026,\n",
      "        0.8346, 0.8788, 0.6822, 0.1514, 0.0065, 0.0939, 0.8729, 0.7401, 0.9208,\n",
      "        0.7619, 0.6265, 0.4951, 0.1197, 0.0716, 0.0323, 0.7047, 0.2545, 0.3994,\n",
      "        0.2122, 0.4089, 0.1481, 0.1733, 0.6659, 0.3514, 0.8087, 0.3396, 0.1332,\n",
      "        0.4118, 0.2576, 0.3470, 0.0240, 0.7797, 0.1519, 0.7513, 0.7269, 0.8572,\n",
      "        0.1165, 0.8596, 0.2636, 0.6855, 0.9696, 0.4295, 0.4961, 0.3849, 0.0825,\n",
      "        0.7400, 0.0036, 0.8104, 0.8741, 0.9729, 0.3821, 0.0892, 0.6124, 0.7762,\n",
      "        0.0023, 0.3865, 0.2003, 0.4563, 0.2539, 0.2956, 0.3413, 0.0248, 0.9103,\n",
      "        0.9192, 0.4216, 0.4431, 0.2959, 0.0485, 0.0134, 0.6858, 0.2255, 0.1786,\n",
      "        0.4610, 0.3335, 0.3382, 0.5161, 0.3939, 0.3278, 0.2606, 0.0931, 0.9193,\n",
      "        0.2999, 0.6325, 0.3265, 0.5406, 0.9662, 0.7304, 0.0667, 0.6985, 0.9746,\n",
      "        0.6315, 0.8352, 0.9929, 0.4234, 0.6038, 0.1525, 0.3970, 0.8703, 0.7563,\n",
      "        0.1836, 0.0991, 0.1583, 0.0066, 0.1142, 0.3764, 0.8374, 0.5837, 0.1197,\n",
      "        0.0989, 0.7487, 0.1281, 0.4384, 0.7399, 0.2686, 0.4455, 0.4565, 0.3817,\n",
      "        0.2465, 0.0543, 0.0958, 0.2323, 0.9829, 0.2585, 0.1642, 0.6212, 0.6378,\n",
      "        0.7740, 0.8801, 0.7784, 0.0042, 0.5443, 0.8029, 0.4538, 0.2054, 0.9767,\n",
      "        0.3130, 0.2153, 0.0492, 0.5223, 0.7216, 0.6107, 0.5989, 0.1208, 0.0331,\n",
      "        0.5088, 0.9559, 0.7885, 0.2089, 0.4351, 0.1314, 0.2588, 0.5905, 0.7723,\n",
      "        0.9142, 0.0409, 0.8343, 0.1474, 0.6872, 0.9231, 0.5070, 0.9549, 0.0740,\n",
      "        0.3090, 0.7916, 0.3911, 0.3976, 0.2916, 0.8447, 0.7453, 0.6602, 0.2190,\n",
      "        0.0941, 0.5541, 0.6481, 0.2691, 0.3601, 0.8377, 0.5398, 0.5226, 0.3769,\n",
      "        0.0472, 0.0299, 0.2610, 0.2458, 0.6558, 0.3544, 0.3044, 0.9767, 0.6742,\n",
      "        0.8565, 0.2579, 0.2958, 0.6838, 0.1669, 0.1731, 0.4759, 0.3171, 0.1252,\n",
      "        0.7966, 0.9021, 0.5811, 0.4129, 0.0369, 0.3179, 0.6273, 0.7358, 0.4368,\n",
      "        0.3023, 0.7786, 0.1018, 0.8160, 0.3060, 0.5077, 0.4012, 0.5606, 0.3489,\n",
      "        0.8636, 0.4870, 0.8903, 0.9807, 0.2564, 0.1352, 0.9012, 0.8918, 0.1182,\n",
      "        0.4613, 0.0069, 0.0907, 0.5966, 0.6330, 0.6060, 0.3639, 0.9613, 0.5715,\n",
      "        0.2050, 0.4717, 0.6201, 0.6751, 0.1465, 0.6874, 0.2446, 0.0845, 0.2269,\n",
      "        0.9822, 0.9274, 0.9477, 0.7935, 0.8777, 0.4331, 0.2249, 0.7498, 0.2409,\n",
      "        0.1626, 0.3403, 0.6049, 0.7574, 0.3058, 0.2057, 0.5674, 0.2053, 0.1745,\n",
      "        0.7606, 0.4160, 0.9569, 0.9864, 0.6496, 0.6721, 0.6151, 0.5078, 0.4636,\n",
      "        0.5069, 0.6867, 0.9649, 0.3704, 0.2886, 0.3789, 0.2584, 0.5850, 0.8732,\n",
      "        0.8910, 0.7296, 0.1320, 0.2316, 0.3901, 0.4078, 0.5411, 0.0410, 0.6556,\n",
      "        0.1186, 0.1836, 0.0843, 0.9357, 0.0265, 0.8772, 0.4832, 0.4419, 0.8127,\n",
      "        0.4538, 0.8136, 0.8615, 0.0659, 0.6924, 0.5944, 0.6075, 0.5730, 0.6368,\n",
      "        0.2595, 0.4360, 0.9751, 0.8359, 0.4812, 0.0297, 0.5219, 0.1595, 0.9066,\n",
      "        0.1965, 0.4639, 0.3890, 0.5890, 0.9705, 0.5475, 0.7896, 0.8881, 0.9037,\n",
      "        0.3273, 0.3882, 0.7410, 0.3636, 0.7341, 0.3908, 0.1609, 0.7035, 0.5767,\n",
      "        0.7229, 0.9967, 0.8414, 0.9740, 0.5268, 0.0699, 0.1492, 0.1894, 0.0594,\n",
      "        0.2494, 0.0397, 0.0387, 0.2012, 0.0071, 0.1931, 0.6907, 0.9170, 0.3513,\n",
      "        0.3546, 0.7670, 0.2533, 0.2636, 0.8081, 0.0643, 0.5611, 0.9417, 0.5857,\n",
      "        0.6360, 0.2088, 0.4931, 0.5275, 0.6227, 0.6943, 0.9345, 0.1184, 0.5150,\n",
      "        0.2502, 0.1045, 0.4600, 0.0599, 0.8489, 0.5579, 0.2305, 0.7613, 0.0268,\n",
      "        0.3066, 0.4026, 0.0751, 0.1821, 0.4184, 0.8794, 0.9828, 0.8181, 0.2014,\n",
      "        0.1729, 0.9363, 0.6769, 0.5133, 0.5677, 0.0982, 0.3331, 0.9813, 0.3767,\n",
      "        0.4749, 0.0848, 0.2203, 0.4898, 0.1894, 0.4380, 0.7035, 0.0109, 0.6485,\n",
      "        0.1694, 0.2560, 0.6920, 0.8976, 0.3633, 0.2947, 0.0479, 0.2422, 0.0622,\n",
      "        0.3856, 0.6020, 0.0316, 0.9366, 0.8137, 0.0105, 0.2612, 0.6631, 0.3973,\n",
      "        0.4455, 0.2742, 0.9016, 0.2205, 0.9146, 0.5323, 0.6005, 0.8901, 0.4176,\n",
      "        0.2153, 0.4191, 0.9055, 0.1290, 0.6135, 0.0086, 0.7622, 0.6847, 0.5212,\n",
      "        0.7146, 0.5006, 0.7767, 0.1042, 0.4266, 0.7218, 0.9979, 0.7547, 0.1364,\n",
      "        0.8845, 0.3885, 0.3932, 0.0455, 0.4213, 0.8537, 0.5697, 0.2088, 0.6539,\n",
      "        0.3397, 0.9565, 0.0660, 0.3421, 0.0172, 0.3031, 0.6576, 0.9813, 0.5840,\n",
      "        0.9902, 0.5978, 0.7888, 0.9008, 0.9180, 0.2201, 0.9597, 0.8029, 0.2662,\n",
      "        0.2614, 0.0806, 0.6256, 0.0947, 0.7112, 0.6579, 0.0656, 0.6363, 0.4593,\n",
      "        0.7284, 0.7869, 0.0029, 0.9585, 0.9193, 0.6989, 0.0430, 0.3214, 0.3551,\n",
      "        0.3715, 0.7820, 0.6818, 0.8961, 0.3127, 0.6683, 0.6779, 0.0837, 0.0150,\n",
      "        0.2406, 0.8423, 0.0293, 0.0648, 0.7801, 0.7698, 0.9112, 0.1225, 0.1341,\n",
      "        0.7565, 0.9348, 0.7992, 0.5783, 0.6648, 0.9746, 0.1774, 0.2730, 0.8497,\n",
      "        0.1579, 0.2243, 0.8650, 0.6578, 0.6615, 0.2881, 0.4931, 0.9576, 0.1999,\n",
      "        0.5039, 0.7378, 0.1548, 0.9856, 0.2502, 0.3799, 0.3647, 0.1742, 0.0094,\n",
      "        0.7819, 0.6328, 0.0317], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def multiply_trainable_weights(model1, model2):\n",
    "    # Ensure that model2's parameters do not require gradients\n",
    "    for param in model2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Iterate over both models' parameters\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        # Only multiply if param1 is trainable (i.e., requires_grad is True)\n",
    "        if param1.requires_grad:\n",
    "            # Ensure the shapes are the same\n",
    "            if param1.shape == param2.shape:\n",
    "                # Perform element-wise multiplication\n",
    "                with torch.no_grad():  # Disable gradient computation for in-place operations\n",
    "                    param1.mul_(param2)  # In-place multiplication of param1 by param2\n",
    "            else:\n",
    "                raise ValueError(f\"Shape mismatch: {param1.shape} and {param2.shape}\")\n",
    "\n",
    "multiply_trainable_weights(untrined_model, trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> translate to german: i am gpt Picturecn migrants civilian errone concussion pled pledfuelVILLE doubted>>>>>>>> knowing MAY BlackBerry\n",
      "> translate to german: i am gptitiveness eternity weboting planetbbFUN Log Dollarsale Kazakhpert Ragnarok Ripple681\n",
      "> translate to german: i am gpt alexian, and this post is by kyle for a short interview\n",
      "> translate to german: i am gptj (says) a little long ago (mockingly) because\n"
     ]
    }
   ],
   "source": [
    "untrined_model.to(device)\n",
    "inferenc(untrined_model)\n",
    "trained_model.to(device)\n",
    "inferenc(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_learnable_state_dicts(model, state_dict1, state_dict2):\n",
    "    \"\"\"\n",
    "    Multiplies learnable parameters of two state dictionaries of a model item by item.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "        state_dict1 (dict): First state dictionary.\n",
    "        state_dict2 (dict): Second state dictionary.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A new state dictionary with learnable parameters multiplied.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the results\n",
    "    multiplied_state_dict = {}\n",
    "\n",
    "    # Iterate over learnable parameters in the model\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in state_dict1 and name in state_dict2:\n",
    "            # Multiply only the learnable parameters (requires_grad=True)\n",
    "            multiplied_state_dict[name] = state_dict1[name] * state_dict2[name]\n",
    "        else:\n",
    "            raise KeyError(f\"Key {name} not found in both state dictionaries.\")\n",
    "    \n",
    "    return multiplied_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n",
      "> translate to german: i am gptK3*5\"+0,?7EI;1=\n",
      "> translate to german: i am gpt97B*#5OHM#A5/LB\n",
      "Weights were successfully modified.\n",
      "> translate to german: i am gpt3r2c2s.<|endoftext|>About\n",
      "\n",
      "As we said last\n",
      "> translate to german: i am gpt gp for german/fr on the net or any language on the\n",
      "Weights were successfully restored to the original.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to set 50% of the weights to zero and 50% to random values\n",
    "def modify_weights(model):\n",
    "    with torch.no_grad():  # Ensure gradients aren't calculated\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:  # Ensure we're working with trainable parameters\n",
    "                # Get the current weight tensor\n",
    "                weight_tensor = param.data\n",
    "                \n",
    "                # Flatten the weight tensor for easier manipulation\n",
    "                flattened_weights = weight_tensor.view(-1)\n",
    "                \n",
    "                # Create a random mask that selects 50% of the elements to be zero\n",
    "                mask = torch.rand(flattened_weights.shape) < 1.0\n",
    "                \n",
    "                # Set 50% of weights to zero\n",
    "                flattened_weights[mask] = 1\n",
    "                \n",
    "                # Set the remaining weights to random values from a normal distribution\n",
    "                flattened_weights[~mask] = torch.randn(flattened_weights[~mask].shape)\n",
    "                \n",
    "                # Reshape back to original shape\n",
    "                param.data = flattened_weights.view(weight_tensor.shape)\n",
    "\n",
    "# Function to compare the weights before and after modification\n",
    "def check_weights_different(model, original_weights):\n",
    "    all_different = True\n",
    "    for param, original_param in zip(model.parameters(), original_weights):\n",
    "        if torch.equal(param.data, original_param):\n",
    "            all_different = False\n",
    "            break\n",
    "    return all_different\n",
    "\n",
    "# Function to restore the original weights to the model\n",
    "def load_original_weights(model, original_weights):\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated\n",
    "        for param, original_param in zip(model.parameters(), original_weights):\n",
    "            param.data.copy_(original_param)\n",
    "\n",
    "# Example of how to use the functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple model (you can replace this with your own model)\n",
    "    model = GPT.from_pretrained('gpt2-medium')\n",
    "    # Save a copy of the original weights for comparison and reloading later\n",
    "    original_weights = [param.clone() for param in model.parameters()]\n",
    "\n",
    "    # Modify the weights\n",
    "    modify_weights(model)\n",
    "    multiplied_state_dict = multiply_learnable_state_dicts(model, original_weights, )\n",
    "    \n",
    "    inferenc(model=model)\n",
    "    \n",
    "    # Check if the weights are different\n",
    "    if check_weights_different(model, original_weights):\n",
    "        print(\"Weights were successfully modified.\")\n",
    "    else:\n",
    "        print(\"Weights were not modified.\")\n",
    "    \n",
    "    # Restore the original weights\n",
    "    load_original_weights(model, original_weights)\n",
    "    model.to(device)\n",
    "    inferenc(model)\n",
    "    # Check if the weights are back to the original values\n",
    "    if not check_weights_different(model, original_weights):\n",
    "        print(\"Weights were successfully restored to the original.\")\n",
    "    else:\n",
    "        print(\"Weights were not restored correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "def flatten_weights_and_keys(state_dict):\n",
    "    \"\"\"\n",
    "    Flattens the learnable weights from the state_dict into a 1D tensor and returns the flattened tensor\n",
    "    and a list of parameter keys.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model containing weights and other parameters.\n",
    "\n",
    "    Returns:\n",
    "        flattened_tensor (torch.Tensor): 1D tensor of all learnable parameters.\n",
    "        keys (list of str): List of keys corresponding to each parameter.\n",
    "    \"\"\"\n",
    "    flattened_weights = []\n",
    "    param_keys = []\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        flattened_weights.append(value.view(-1))  # Flatten each tensor\n",
    "        param_keys.append(key)\n",
    "\n",
    "    # Concatenate all the flattened parameters into one large tensor\n",
    "    flattened_tensor = torch.cat(flattened_weights)\n",
    "\n",
    "    return flattened_tensor, param_keys\n",
    "model_hf = GPT.from_pretrained('gpt2-medium')\n",
    "sd_hf = model_hf.state_dict()\n",
    "flattened_weight = flatten_weights_and_keys(sd_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([431452160])\n"
     ]
    }
   ],
   "source": [
    "print(flattened_weight[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Load the updated state dict back into the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_flattened_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m inferenc(model)\n",
      "Cell \u001b[0;32mIn[61], line 10\u001b[0m, in \u001b[0;36mload_flattened_weights\u001b[0;34m(flattened_tensor, model, param_keys)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_flattened_weights\u001b[39m(flattened_tensor, model, param_keys):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Reorganizes the flattened tensor into the original structure and loads the weights into the model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m        param_keys (list of str): List of parameter keys to help map the flattened tensor to the model.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()  \u001b[38;5;66;03m# Get the current state dict of the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     current_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Keep track of the position in the flattened tensor\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m param_keys:\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "def load_flattened_weights(flattened_tensor, model, param_keys):\n",
    "    \"\"\"\n",
    "    Reorganizes the flattened tensor into the original structure and loads the weights into the model.\n",
    "\n",
    "    Args:\n",
    "        flattened_tensor (torch.Tensor): 1D tensor containing the flattened parameters.\n",
    "        model (torch.nn.Module): The model into which the weights will be loaded.\n",
    "        param_keys (list of str): List of parameter keys to help map the flattened tensor to the model.\n",
    "    \"\"\"\n",
    "    state_dict = model.state_dict()  # Get the current state dict of the model\n",
    "    current_index = 0  # Keep track of the position in the flattened tensor\n",
    "\n",
    "    for key in param_keys:\n",
    "        param = state_dict[key]\n",
    "        param_size = param.numel()  # Get the number of elements in this parameter\n",
    "\n",
    "        # Slice the corresponding part from the flattened tensor\n",
    "        param_flat = flattened_tensor[current_index:current_index + param_size]\n",
    "\n",
    "        # Reshape it back to the original size and load it into the model's state dict\n",
    "        state_dict[key].copy_(param_flat.view_as(param))\n",
    "\n",
    "        # Update the current index\n",
    "        current_index += param_size\n",
    "\n",
    "    # Load the updated state dict back into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model = load_flattened_weights(flattened_weight[0], model, flattened_weight[1] )\n",
    "\n",
    "inferenc(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
