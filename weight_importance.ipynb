{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPT, GPTConfig\n",
    "device = 'cuda'\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "source": [
    "trained_model = GPT.from_pretrained('gpt2')\n",
    "untrined_model = model =  GPT(GPTConfig(vocab_size=50257))\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "max_length = 25\n",
    "number_return_sequences = 2\n",
    "\n",
    "def inferenc(model):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = 'translate to german: i am gpt'\n",
    "    tokens = enc.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(number_return_sequences, 1)\n",
    "    x = tokens.to(device)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    while x.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0] # B, T, vocab_size\n",
    "            logits = logits[:, -1, :] # (B, vocab-size)\n",
    "            # got the probabilities\n",
    "            probs = F.softmax(logits, dim=-1 )\n",
    "            # get prbablities\n",
    "            topk_probs, topk_indicies =  torch.topk(probs, 50, dim=-1) # (5, 50)\n",
    "            # Select top-k proababilits \n",
    "            ix = torch.multinomial(topk_probs, 1) #(B, 1)\n",
    "            # gather the corresponding indecies\n",
    "            xcol = torch.gather(topk_indicies, -1, ix) # (B, 1)\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "        # print inducidual raws of output \n",
    "    for i in range(number_return_sequences):\n",
    "        tokens = x [ i , :max_length].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(\">\", decoded)\n",
    "    \n",
    "#inferenc(trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_specific_layer(model, tensor, layer_name):\n",
    "    # Check if the layer exists in the model\n",
    "    if layer_name not in dict(model.named_parameters()):\n",
    "        raise ValueError(f\"Layer {layer_name} not found in the model\")\n",
    "\n",
    "    # Get the layer parameter (e.g., weight or bias)\n",
    "    layer_param = dict(model.named_parameters())[layer_name]\n",
    "\n",
    "    # Ensure the shapes of the layer and the tensor are compatible\n",
    "    if layer_param.shape != tensor.shape:\n",
    "        raise ValueError(f\"Shape mismatch: Layer shape {layer_param.shape}, Tensor shape {tensor.shape}\")\n",
    "\n",
    "    # Perform the element-wise multiplication with the tensor\n",
    "    with torch.no_grad():  # Ensure no gradients are tracked\n",
    "        layer_param.mul_(tensor)  # In-place multiplication of the layer's weight\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> translate to german: i am gptv: Gpt: http://www.kern.de gpt\n",
      "> translate to german: i am gpt for an in german: i hate gt /.\n",
      "\n",
      "Anonymous\n"
     ]
    }
   ],
   "source": [
    "model = trained_model   # Assuming this is your model\n",
    "tensor = torch.zeros_like(model.transformer.h[1].ln_2.weight)  # Example tensor with matching shape\n",
    "\n",
    "layer_name = \"transformer.h.1.ln_2.weight\"  # The layer we want to modify\n",
    "updated_model = multiply_specific_layer(model, tensor, layer_name)\n",
    "inferenc(model.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GPTprover(nn.Module):\n",
    "    def __init__(self, model, dict_key) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_name = dict_key\n",
    "        self.l1 = nn.Parameter(torch.ones_like(model.state_dict()[dict_key].shape))\n",
    "        \n",
    "    def forward(self, x, y=None): \n",
    "        new_model = multiply_specific_layer(self.model, self.l1, self.layer_name)\n",
    "        new_model.eval()\n",
    "        logit, loss = new_model(x, y)\n",
    "        loss = 0.5 * loss + 0.5 * torch.sum(torch.abs(self.l1))\n",
    "        return logit, loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "Step 0 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  232.12ms | tok/sec: 35292.19341008793\n",
      "Step 1 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.57ms | tok/sec: 41463.516857051145\n",
      "Step 2 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.80ms | tok/sec: 41415.38813722146\n",
      "Step 3 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.64ms | tok/sec: 41240.80850844207\n",
      "Step 4 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.29ms | tok/sec: 41106.65603659908\n",
      "Step 5 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.38ms | tok/sec: 41295.18151269392\n",
      "Step 6 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.01ms | tok/sec: 41371.45536640988\n",
      "Step 7 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.08ms | tok/sec: 41356.46677314794\n",
      "Step 8 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.07ms | tok/sec: 41780.54875447782\n",
      "Step 9 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.52ms | tok/sec: 41265.82112883777\n",
      "Step 10 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.77ms | tok/sec: 41214.29360969671\n",
      "Step 11 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.36ms | tok/sec: 41298.1099215979\n",
      "Step 12 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.56ms | tok/sec: 41465.76859849221\n",
      "Step 13 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.19ms | tok/sec: 41334.128545787884\n",
      "Step 14 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.69ms | tok/sec: 41229.47614167881\n",
      "Step 15 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.00ms | tok/sec: 41373.59748095969\n",
      "Step 16 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.41ms | tok/sec: 41287.98633977291\n",
      "Step 17 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.27ms | tok/sec: 41317.080182922065\n",
      "Step 18 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  201.06ms | tok/sec: 40744.769745236525\n",
      "Step 19 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.56ms | tok/sec: 41256.852721760275\n",
      "Step 20 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.72ms | tok/sec: 41854.75263116863\n",
      "Step 21 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.63ms | tok/sec: 41874.544041728615\n",
      "Step 22 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.47ms | tok/sec: 41908.35510649145\n",
      "Step 23 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.98ms | tok/sec: 41800.11747917582\n",
      "Step 24 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.72ms | tok/sec: 41432.11805590726\n",
      "Step 25 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.87ms | tok/sec: 41192.35558049175\n",
      "Step 26 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.60ms | tok/sec: 41248.531942045134\n",
      "Step 27 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.15ms | tok/sec: 41343.23007634598\n",
      "Step 28 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.74ms | tok/sec: 41427.72202220909\n",
      "Step 29 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.76ms | tok/sec: 41216.07339299873\n",
      "Step 30 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.49ms | tok/sec: 41271.52131397225\n",
      "Step 31 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.20ms | tok/sec: 41332.63687390082\n",
      "Step 32 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.55ms | tok/sec: 41260.12251803943\n",
      "Step 33 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.03ms | tok/sec: 41366.673771451\n",
      "Step 34 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.41ms | tok/sec: 41287.837500600814\n",
      "Step 35 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.29ms | tok/sec: 41312.90721718899\n",
      "Step 36 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.99ms | tok/sec: 41374.84299837438\n",
      "Step 37 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.20ms | tok/sec: 41331.592767637936\n",
      "Step 38 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.73ms | tok/sec: 41431.168833298565\n",
      "Step 39 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.54ms | tok/sec: 41260.717082138995\n",
      "Step 40 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.65ms | tok/sec: 41238.482143457244\n",
      "Step 41 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.77ms | tok/sec: 41213.65094794518\n",
      "Step 42 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.53ms | tok/sec: 41262.94685013366\n",
      "Step 43 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.99ms | tok/sec: 41376.487196191796\n",
      "Step 44 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.39ms | tok/sec: 41292.55126216042\n",
      "Step 45 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.81ms | tok/sec: 41205.989986244575\n",
      "Step 46 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.71ms | tok/sec: 41226.60692362701\n",
      "Step 47 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.27ms | tok/sec: 41317.42796811944\n",
      "Step 48 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.72ms | tok/sec: 41224.52946328517\n",
      "Step 49 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.74ms | tok/sec: 41220.22681854445\n",
      "Step 50 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.47ms | tok/sec: 41276.52886255628\n",
      "Step 51 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.39ms | tok/sec: 41292.501638012465\n",
      "Step 52 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.91ms | tok/sec: 41602.531002960386\n",
      "Step 53 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.11ms | tok/sec: 41351.290514283515\n",
      "Step 54 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.57ms | tok/sec: 41254.92081300188\n",
      "Step 55 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.41ms | tok/sec: 41288.68093676511\n",
      "Step 56 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.45ms | tok/sec: 41279.80177855308\n",
      "Step 57 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.69ms | tok/sec: 40819.943436148256\n",
      "Step 58 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.01ms | tok/sec: 41371.256111216804\n",
      "Step 59 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.19ms | tok/sec: 41755.00993208076\n",
      "Step 60 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.94ms | tok/sec: 41808.61143247907\n",
      "Step 61 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.06ms | tok/sec: 41783.800467456786\n",
      "Step 62 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.55ms | tok/sec: 41891.034162910546\n",
      "Step 63 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  201.29ms | tok/sec: 40698.10137410439\n",
      "Step 64 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.89ms | tok/sec: 41189.39277310058\n",
      "Step 65 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.81ms | tok/sec: 41205.34758342468\n",
      "Step 66 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.38ms | tok/sec: 41294.28818248465\n",
      "Step 67 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.25ms | tok/sec: 41321.60184721955\n",
      "Step 68 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.37ms | tok/sec: 41297.0179239557\n",
      "Step 69 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.19ms | tok/sec: 41334.377168236766\n",
      "Step 70 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.42ms | tok/sec: 41285.80347279213\n",
      "Step 71 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.45ms | tok/sec: 41278.95870510909\n",
      "Step 72 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.17ms | tok/sec: 41338.60420752234\n",
      "Step 73 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.27ms | tok/sec: 41317.92481427235\n",
      "Step 74 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.69ms | tok/sec: 41229.674033507166\n",
      "Step 75 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.81ms | tok/sec: 41205.34758342468\n",
      "Step 76 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.54ms | tok/sec: 41260.46934501508\n",
      "Step 77 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.36ms | tok/sec: 41298.95377751281\n",
      "Step 78 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.22ms | tok/sec: 41327.0192095125\n",
      "Step 79 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.53ms | tok/sec: 41264.03704664457\n",
      "Step 80 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.02ms | tok/sec: 41161.119418468974\n",
      "Step 81 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.44ms | tok/sec: 41282.48000499815\n",
      "Step 82 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.24ms | tok/sec: 41324.33520112091\n",
      "Step 83 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.16ms | tok/sec: 41340.54396196063\n",
      "Step 84 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.65ms | tok/sec: 41238.828606714684\n",
      "Step 85 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.31ms | tok/sec: 41102.27962443044\n",
      "Step 86 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.64ms | tok/sec: 41240.51151106337\n",
      "Step 87 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.35ms | tok/sec: 41300.244447382654\n",
      "Step 88 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.75ms | tok/sec: 41218.29833829972\n",
      "Step 89 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.36ms | tok/sec: 41298.75522007741\n",
      "Step 90 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.24ms | tok/sec: 41116.44486991859\n",
      "Step 91 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.64ms | tok/sec: 41240.6105097143\n",
      "Step 92 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.05ms | tok/sec: 41155.44968660054\n",
      "Step 93 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.26ms | tok/sec: 41318.57073213554\n",
      "Step 94 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.23ms | tok/sec: 41325.677161736654\n",
      "Step 95 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.73ms | tok/sec: 41014.40811317962\n",
      "Step 96 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.57ms | tok/sec: 40843.47696177373\n",
      "Step 97 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.43ms | tok/sec: 41284.61291535749\n",
      "Step 98 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.57ms | tok/sec: 41254.078755548515\n",
      "Step 99 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.70ms | tok/sec: 41228.83300635596\n",
      "Step 100 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.21ms | tok/sec: 41123.3342804379\n",
      "Step 101 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.44ms | tok/sec: 41282.0336097905\n",
      "Step 102 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.14ms | tok/sec: 41345.07076366227\n",
      "Step 103 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.57ms | tok/sec: 41254.82174563495\n",
      "Step 104 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.62ms | tok/sec: 41244.42231921217\n",
      "Step 105 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.78ms | tok/sec: 41210.93221613595\n",
      "Step 106 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.36ms | tok/sec: 40886.92746917988\n",
      "Step 107 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.40ms | tok/sec: 41291.06258960385\n",
      "Step 108 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.90ms | tok/sec: 41816.650806400576\n",
      "Step 109 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.83ms | tok/sec: 40994.39409084456\n",
      "Step 110 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.05ms | tok/sec: 41784.41021999003\n",
      "Step 111 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.26ms | tok/sec: 41112.951836695625\n",
      "Step 112 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.51ms | tok/sec: 41686.974502324585\n",
      "Step 113 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.93ms | tok/sec: 41810.29028682126\n",
      "Step 114 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.11ms | tok/sec: 41773.13267812073\n",
      "Step 115 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.86ms | tok/sec: 41194.52857682371\n",
      "Step 116 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.06ms | tok/sec: 41153.7737171986\n",
      "Step 117 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.75ms | tok/sec: 41218.199446741026\n",
      "Step 118 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.59ms | tok/sec: 41458.66393491085\n",
      "Step 119 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.07ms | tok/sec: 41568.155318006065\n",
      "Step 120 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.41ms | tok/sec: 41288.08556648382\n",
      "Step 121 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.27ms | tok/sec: 41109.9020677146\n",
      "Step 122 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.00ms | tok/sec: 41584.15332518426\n",
      "Step 123 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.48ms | tok/sec: 41067.20367743115\n",
      "Step 124 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.30ms | tok/sec: 41103.36134747319\n",
      "Step 125 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.48ms | tok/sec: 41065.87845076718\n",
      "Step 126 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.73ms | tok/sec: 41014.995616757544\n",
      "Step 127 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.25ms | tok/sec: 40908.78482124892\n",
      "Step 128 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.54ms | tok/sec: 41471.023613075784\n",
      "Step 129 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  201.23ms | tok/sec: 40709.28817206085\n",
      "Step 130 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.19ms | tok/sec: 41125.74611959592\n",
      "Step 131 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.51ms | tok/sec: 41901.45566960524\n",
      "Step 132 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.98ms | tok/sec: 41378.729494732484\n",
      "Step 133 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.40ms | tok/sec: 41498.822260253706\n",
      "Step 134 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.13ms | tok/sec: 41768.308562690014\n",
      "Step 135 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.55ms | tok/sec: 41051.40469275131\n",
      "Step 136 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.16ms | tok/sec: 41762.16577615503\n",
      "Step 137 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  196.01ms | tok/sec: 41793.91449697916\n",
      "Step 138 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.10ms | tok/sec: 41988.965430940094\n",
      "Step 139 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.40ms | tok/sec: 41498.77213908717\n",
      "Step 140 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.85ms | tok/sec: 41826.933497590915\n",
      "Step 141 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.72ms | tok/sec: 41855.05854149359\n",
      "Step 142 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.48ms | tok/sec: 41906.61725648606\n",
      "Step 143 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  197.94ms | tok/sec: 41386.40497796964\n",
      "Step 144 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.00ms | tok/sec: 41164.817911384744\n",
      "Step 145 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.31ms | tok/sec: 41102.08295363061\n",
      "Step 146 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.25ms | tok/sec: 41113.197805524476\n",
      "Step 147 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  199.19ms | tok/sec: 41126.435268460955\n",
      "Step 148 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.92ms | tok/sec: 41181.39532163322\n",
      "Step 149 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.92ms | tok/sec: 41182.57993328755\n",
      "Step 150 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.05ms | tok/sec: 40950.32455167038\n",
      "Step 151 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  200.18ms | tok/sec: 40923.01194708552\n",
      "Step 152 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  198.00ms | tok/sec: 41373.34838647529\n",
      "Step 153 | loss nan | lr  3.0000e-04  | norm :  nan |  dt:  195.82ms | tok/sec: 41833.299082363286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from gpt import DataLoaderLight\n",
    "train_loader = DataLoaderLight(B=8, T=1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-10, betas=(0.9, 0.95), eps=1e-8)\n",
    "#optimizer = model.configure_optimizer(weight_decay=0.1, learning_rate = 6e-4, device=device)\n",
    "model = GPTprover(trained_model, \"transformer.h.1.ln_2.weight\")\n",
    "model.to(device)\n",
    "for step in range(500):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "        logits, loss = model(x, y)\n",
    "        #import code; code.interact(local=locals())\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = 3e-4\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 -t0 ) * 1000 # time difference in milisecond\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T)/ (t1-t0)\n",
    "    print(f\"Step {step} | loss {loss.item()} | lr {lr: .4e}  | norm : {norm: .4f} |  dt: {dt: .2f}ms | tok/sec: {tokens_per_sec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inferenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def multiply_trainable_weights(model1, model2):\n",
    "    # Ensure that model2's parameters do not require gradients\n",
    "    for param in model2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Iterate over both models' parameters\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        # Only multiply if param1 is trainable (i.e., requires_grad is True)\n",
    "        if param1.requires_grad:\n",
    "            # Ensure the shapes are the same\n",
    "            if param1.shape == param2.shape:\n",
    "                # Perform element-wise multiplication\n",
    "                with torch.no_grad():  # Disable gradient computation for in-place operations\n",
    "                    param1.mul_(param2)  # In-place multiplication of param1 by param2\n",
    "            else:\n",
    "                raise ValueError(f\"Shape mismatch: {param1.shape} and {param2.shape}\")\n",
    "\n",
    "multiply_trainable_weights(untrined_model, trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> translate to german: i am gpt Picturecn migrants civilian errone concussion pled pledfuelVILLE doubted>>>>>>>> knowing MAY BlackBerry\n",
      "> translate to german: i am gptitiveness eternity weboting planetbbFUN Log Dollarsale Kazakhpert Ragnarok Ripple681\n",
      "> translate to german: i am gpt alexian, and this post is by kyle for a short interview\n",
      "> translate to german: i am gptj (says) a little long ago (mockingly) because\n"
     ]
    }
   ],
   "source": [
    "untrined_model.to(device)\n",
    "inferenc(untrined_model)\n",
    "trained_model.to(device)\n",
    "inferenc(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_learnable_state_dicts(model, state_dict1, state_dict2):\n",
    "    \"\"\"\n",
    "    Multiplies learnable parameters of two state dictionaries of a model item by item.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "        state_dict1 (dict): First state dictionary.\n",
    "        state_dict2 (dict): Second state dictionary.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A new state dictionary with learnable parameters multiplied.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold the results\n",
    "    multiplied_state_dict = {}\n",
    "\n",
    "    # Iterate over learnable parameters in the model\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in state_dict1 and name in state_dict2:\n",
    "            # Multiply only the learnable parameters (requires_grad=True)\n",
    "            multiplied_state_dict[name] = state_dict1[name] * state_dict2[name]\n",
    "        else:\n",
    "            raise KeyError(f\"Key {name} not found in both state dictionaries.\")\n",
    "    \n",
    "    return multiplied_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n",
      "> translate to german: i am gptK3*5\"+0,?7EI;1=\n",
      "> translate to german: i am gpt97B*#5OHM#A5/LB\n",
      "Weights were successfully modified.\n",
      "> translate to german: i am gpt3r2c2s.<|endoftext|>About\n",
      "\n",
      "As we said last\n",
      "> translate to german: i am gpt gp for german/fr on the net or any language on the\n",
      "Weights were successfully restored to the original.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to set 50% of the weights to zero and 50% to random values\n",
    "def modify_weights(model):\n",
    "    with torch.no_grad():  # Ensure gradients aren't calculated\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:  # Ensure we're working with trainable parameters\n",
    "                # Get the current weight tensor\n",
    "                weight_tensor = param.data\n",
    "                \n",
    "                # Flatten the weight tensor for easier manipulation\n",
    "                flattened_weights = weight_tensor.view(-1)\n",
    "                \n",
    "                # Create a random mask that selects 50% of the elements to be zero\n",
    "                mask = torch.rand(flattened_weights.shape) < 1.0\n",
    "                \n",
    "                # Set 50% of weights to zero\n",
    "                flattened_weights[mask] = 1\n",
    "                \n",
    "                # Set the remaining weights to random values from a normal distribution\n",
    "                flattened_weights[~mask] = torch.randn(flattened_weights[~mask].shape)\n",
    "                \n",
    "                # Reshape back to original shape\n",
    "                param.data = flattened_weights.view(weight_tensor.shape)\n",
    "\n",
    "# Function to compare the weights before and after modification\n",
    "def check_weights_different(model, original_weights):\n",
    "    all_different = True\n",
    "    for param, original_param in zip(model.parameters(), original_weights):\n",
    "        if torch.equal(param.data, original_param):\n",
    "            all_different = False\n",
    "            break\n",
    "    return all_different\n",
    "\n",
    "# Function to restore the original weights to the model\n",
    "def load_original_weights(model, original_weights):\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated\n",
    "        for param, original_param in zip(model.parameters(), original_weights):\n",
    "            param.data.copy_(original_param)\n",
    "\n",
    "# Example of how to use the functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple model (you can replace this with your own model)\n",
    "    model = GPT.from_pretrained('gpt2-medium')\n",
    "    # Save a copy of the original weights for comparison and reloading later\n",
    "    original_weights = [param.clone() for param in model.parameters()]\n",
    "\n",
    "    # Modify the weights\n",
    "    modify_weights(model)\n",
    "    multiplied_state_dict = multiply_learnable_state_dicts(model, original_weights, )\n",
    "    \n",
    "    inferenc(model=model)\n",
    "    \n",
    "    # Check if the weights are different\n",
    "    if check_weights_different(model, original_weights):\n",
    "        print(\"Weights were successfully modified.\")\n",
    "    else:\n",
    "        print(\"Weights were not modified.\")\n",
    "    \n",
    "    # Restore the original weights\n",
    "    load_original_weights(model, original_weights)\n",
    "    model.to(device)\n",
    "    inferenc(model)\n",
    "    # Check if the weights are back to the original values\n",
    "    if not check_weights_different(model, original_weights):\n",
    "        print(\"Weights were successfully restored to the original.\")\n",
    "    else:\n",
    "        print(\"Weights were not restored correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n"
     ]
    }
   ],
   "source": [
    "def flatten_weights_and_keys(state_dict):\n",
    "    \"\"\"\n",
    "    Flattens the learnable weights from the state_dict into a 1D tensor and returns the flattened tensor\n",
    "    and a list of parameter keys.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model containing weights and other parameters.\n",
    "\n",
    "    Returns:\n",
    "        flattened_tensor (torch.Tensor): 1D tensor of all learnable parameters.\n",
    "        keys (list of str): List of keys corresponding to each parameter.\n",
    "    \"\"\"\n",
    "    flattened_weights = []\n",
    "    param_keys = []\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        flattened_weights.append(value.view(-1))  # Flatten each tensor\n",
    "        param_keys.append(key)\n",
    "\n",
    "    # Concatenate all the flattened parameters into one large tensor\n",
    "    flattened_tensor = torch.cat(flattened_weights)\n",
    "\n",
    "    return flattened_tensor, param_keys\n",
    "model_hf = GPT.from_pretrained('gpt2-medium')\n",
    "sd_hf = model_hf.state_dict()\n",
    "flattened_weight = flatten_weights_and_keys(sd_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([431452160])\n"
     ]
    }
   ],
   "source": [
    "print(flattened_weight[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Load the updated state dict back into the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_flattened_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m inferenc(model)\n",
      "Cell \u001b[0;32mIn[61], line 10\u001b[0m, in \u001b[0;36mload_flattened_weights\u001b[0;34m(flattened_tensor, model, param_keys)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_flattened_weights\u001b[39m(flattened_tensor, model, param_keys):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Reorganizes the flattened tensor into the original structure and loads the weights into the model.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m        param_keys (list of str): List of parameter keys to help map the flattened tensor to the model.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()  \u001b[38;5;66;03m# Get the current state dict of the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     current_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Keep track of the position in the flattened tensor\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m param_keys:\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "def load_flattened_weights(flattened_tensor, model, param_keys):\n",
    "    \"\"\"\n",
    "    Reorganizes the flattened tensor into the original structure and loads the weights into the model.\n",
    "\n",
    "    Args:\n",
    "        flattened_tensor (torch.Tensor): 1D tensor containing the flattened parameters.\n",
    "        model (torch.nn.Module): The model into which the weights will be loaded.\n",
    "        param_keys (list of str): List of parameter keys to help map the flattened tensor to the model.\n",
    "    \"\"\"\n",
    "    state_dict = model.state_dict()  # Get the current state dict of the model\n",
    "    current_index = 0  # Keep track of the position in the flattened tensor\n",
    "\n",
    "    for key in param_keys:\n",
    "        param = state_dict[key]\n",
    "        param_size = param.numel()  # Get the number of elements in this parameter\n",
    "\n",
    "        # Slice the corresponding part from the flattened tensor\n",
    "        param_flat = flattened_tensor[current_index:current_index + param_size]\n",
    "\n",
    "        # Reshape it back to the original size and load it into the model's state dict\n",
    "        state_dict[key].copy_(param_flat.view_as(param))\n",
    "\n",
    "        # Update the current index\n",
    "        current_index += param_size\n",
    "\n",
    "    # Load the updated state dict back into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model = load_flattened_weights(flattened_weight[0], model, flattened_weight[1] )\n",
    "\n",
    "inferenc(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
